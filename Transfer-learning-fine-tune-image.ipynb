{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"private_outputs":true,"provenance":[],"authorship_tag":"ABX9TyP6pAyPZdZH1LBPsPwris4m"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# Transfer learning and fine tuning an image classifier with Tensorflow"],"metadata":{"id":"76hUrJLtBx0V"}},{"cell_type":"markdown","source":["A pre-trained model is a saved network that was previously trained on a large dataset, typically on a large-scale image-classification task. You either use the pretrained model as is or use transfer learning to customize this model to a given task.\n","\n","The intuition behind transfer learning for image classification is that if a model is trained on a large and general enough dataset, this model will effectively serve as a generic model of the visual world. You can then take advantage of these learned feature maps without having to start from scratch by training a large model on a large dataset."],"metadata":{"id":"I7hyPhHsCFVx"}},{"cell_type":"markdown","source":["## Loading the libraries"],"metadata":{"id":"ZECc4_JpCOv0"}},{"cell_type":"code","source":["!pip install tqdm"],"metadata":{"id":"mhlvqJm_CvST"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"U3gyyGlX5KNb"},"outputs":[],"source":["import tensorflow as tf"]},{"cell_type":"code","source":["# Check tensorflow version\n","print(tf.__version__)"],"metadata":{"id":"RlY6KlMC5y7V"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import os\n","import zipfile\n","import numpy as np\n","import matplotlib.pyplot as plt\n","\n","from tqdm import tqdm_notebook\n","from tensorflow.keras.preprocessing.image import ImageDataGenerator\n","\n","%matplotlib inline\n"],"metadata":{"id":"TNDsVEH953vk"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Step 1: Data preprocessing"],"metadata":{"id":"ybQR29XiDBlz"}},{"cell_type":"markdown","source":["In this tutorial, you will use a dataset containing several thousand images of cats and dogs. Download and extract a zip file containing the images, then create a tf.data.Dataset for training and validation using the tf.keras.utils.image_dataset_from_directory utility."],"metadata":{"id":"dxmG4ZvEDFVI"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"qqYnlXtGBOGZ"},"outputs":[],"source":["_URL = 'https://storage.googleapis.com/mledu-datasets/cats_and_dogs_filtered.zip'\n","path_to_zip = tf.keras.utils.get_file('cats_and_dogs.zip', origin=_URL, extract=True)\n","PATH = os.path.join(os.path.dirname(path_to_zip), 'cats_and_dogs_filtered')"]},{"cell_type":"code","source":["# Set the training and validation data folders\n","train_dir = os.path.join(PATH, 'train')\n","validation_dir = os.path.join(PATH, 'validation')"],"metadata":{"id":"gEZK8lNmB7Dr"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["BATCH_SIZE=32\n","VAL_BATCH_SIZE=8\n","\n","IMG_SIZE = (160, 160)\n","IMG_SHAPE = IMG_SIZE + (3,)"],"metadata":{"id":"tCRDmqo7Mn2s"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Create Data generator for Data Augmentation"],"metadata":{"id":"R_BY5z7GGonT"}},{"cell_type":"markdown","source":["When you don't have a large image dataset, it's a good practice to artificially introduce sample diversity by applying random, yet realistic, transformations to the training images, such as rotation and horizontal flipping. This helps expose the model to different aspects of the training data and reduce overfitting."],"metadata":{"id":"-0Xk6vYeG0jh"}},{"cell_type":"code","source":["data_gen_train = ImageDataGenerator(horizontal_flip=True, rotation_range=20)\n","data_gen_test = ImageDataGenerator(validation_split=0.5)"],"metadata":{"id":"iccM0XUYEGmq"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["train_generator = data_gen_train.flow_from_directory(train_dir, target_size=IMG_SIZE, batch_size=BATCH_SIZE, class_mode=\"binary\")\n","valid_generator = data_gen_test.flow_from_directory(validation_dir, target_size=IMG_SIZE, batch_size=VAL_BATCH_SIZE, class_mode=\"binary\", subset='training')\n","test_generator = data_gen_test.flow_from_directory(validation_dir, target_size=IMG_SIZE, batch_size=BATCH_SIZE, class_mode=\"binary\", subset='validation')"],"metadata":{"id":"ggLBra4mMu2S"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Build the model"],"metadata":{"id":"l8jwODH-Ob8M"}},{"cell_type":"markdown","source":["You will create the base model from the MobileNet V2 model developed at Google. This is pre-trained on the ImageNet dataset, a large dataset consisting of 1.4M images and 1000 classes. ImageNet is a research training dataset with a wide variety of categories like jackfruit and syringe. This base of knowledge will help us classify cats and dogs from our specific dataset."],"metadata":{"id":"QEmHbGT6Or-K"}},{"cell_type":"markdown","source":["###  Rescale pixel values\n","\n","In a moment, you will download tf.keras.applications.MobileNetV2 for use as your base model. This model expects pixel values in [-1, 1], but at this point, the pixel values in your images are in [0, 255]. To rescale them, use the preprocessing method included with the model.\n","\n"],"metadata":{"id":"cgBUkYhrS4Pu"}},{"cell_type":"code","source":["preprocess_input = tf.keras.applications.mobilenet_v2.preprocess_input"],"metadata":{"id":"IYrFwKmWS7dl"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["First, instantiate a MobileNet V2 model pre-loaded with weights trained on ImageNet. By specifying the include_top=False argument, you load a network that doesn't include the classification layers at the top, which is ideal for feature extraction."],"metadata":{"id":"a1cYzOP7PMeY"}},{"cell_type":"code","source":["#Load the base model\n","base_model = tf.keras.applications.MobileNetV2(input_shape=IMG_SHAPE, include_top=False, weights=\"imagenet\")\n","base_model.summary()"],"metadata":{"id":"z8gItzijOxFF"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Lets check the base model outputs:"],"metadata":{"id":"AfuSCftvQOOl"}},{"cell_type":"code","source":["image_batch, label_batch = next(iter(train_generator))\n","feature_batch = base_model(image_batch)\n","print(feature_batch.shape)"],"metadata":{"id":"OPlwJXa4QTak"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Frezee the base model"],"metadata":{"id":"ISsceOQ0PqGF"}},{"cell_type":"markdown","source":["It is important to freeze the convolutional base before you compile and train the model. Freezing (by setting layer.trainable = False) prevents the weights in a given layer from being updated during training. MobileNet V2 has many layers, so setting the entire model's trainable flag to False will freeze all of them."],"metadata":{"id":"wT0oN04BR4LB"}},{"cell_type":"code","source":["base_model.trainable = False"],"metadata":{"id":"Wlp2QJ8_Pfya"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Add a classification head"],"metadata":{"id":"jYLOBySbP2Z5"}},{"cell_type":"markdown","source":["To generate predictions from the block of features, average over the spatial 5x5 spatial locations, using a tf.keras.layers.GlobalAveragePooling2D layer to convert the features to a single 1280-element vector per image"],"metadata":{"id":"BbZKR9W6P8iw"}},{"cell_type":"code","source":["global_average_layer = tf.keras.layers.GlobalAveragePooling2D()"],"metadata":{"id":"wjJQwZN_Pvc1"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["feature_batch_average = global_average_layer(feature_batch)\n","print(feature_batch_average.shape)"],"metadata":{"id":"Tnq10zSOQ1_v"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Apply a tf.keras.layers.Dense layer to convert these features into a single prediction per image. You don't need an activation function here because this prediction will be treated as a logit, or a raw prediction value. Positive numbers predict class 1, negative numbers predict class 0."],"metadata":{"id":"cPFi1YB1Qgdl"}},{"cell_type":"code","source":["prediction_layer = tf.keras.layers.Dense(units=1, activation='sigmoid')"],"metadata":{"id":"qX81ZuRqQhJH"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["prediction_batch = prediction_layer(feature_batch_average)\n","print(prediction_batch.shape)"],"metadata":{"id":"HUwd4WbGQoNj"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Build a model by chaining together the rescaling, base_model and feature extractor layers using the Keras Functional API. As previously mentioned, use training=False as our model contains a BatchNormalization layer."],"metadata":{"id":"neDSQIooSfuB"}},{"cell_type":"code","source":["IMG_SHAPE"],"metadata":{"id":"M8kJHvs4SrSG"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#Set input layer\n","inputs = tf.keras.Input(shape=IMG_SHAPE)\n","# Rescaling the input\n","x = preprocess_input(inputs)\n","# Apply base model\n","x = base_model(x, training=False)\n","# Set Classification layers\n","x = global_average_layer(x)\n","x = tf.keras.layers.Dropout(0.2)(x)\n","outputs = prediction_layer(x)\n","# Create the model\n","model = tf.keras.Model(inputs, outputs)"],"metadata":{"id":"EZM0s4tgQw2x"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Compile the model"],"metadata":{"id":"PIGYHVBaTmf8"}},{"cell_type":"code","source":["base_learning_rate = 0.0001\n","model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=base_learning_rate),\n","              loss=\"binary_crossentropy\",\n","              metrics=['accuracy'])"],"metadata":{"id":"7ypBFNtgTfCW"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### TRain the model"],"metadata":{"id":"eSnf4aCuUIYR"}},{"cell_type":"code","source":["history= model.fit_generator(train_generator, epochs=4, validation_data=valid_generator)"],"metadata":{"id":"BhmuKF7wUE9c"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Plot the learning curves"],"metadata":{"id":"7kfrABAjVfnU"}},{"cell_type":"code","source":["acc = history.history['accuracy']\n","val_acc = history.history['val_accuracy']\n","\n","loss = history.history['loss']\n","val_loss = history.history['val_loss']\n","\n","plt.figure(figsize=(8, 8))\n","plt.subplot(2, 1, 1)\n","plt.plot(acc, label='Training Accuracy')\n","plt.plot(val_acc, label='Validation Accuracy')\n","plt.legend(loc='lower right')\n","plt.ylabel('Accuracy')\n","plt.ylim([min(plt.ylim()),1])\n","plt.title('Training and Validation Accuracy')\n","\n","plt.subplot(2, 1, 2)\n","plt.plot(loss, label='Training Loss')\n","plt.plot(val_loss, label='Validation Loss')\n","plt.legend(loc='upper right')\n","plt.ylabel('Cross Entropy')\n","plt.ylim([0,1.0])\n","plt.title('Training and Validation Loss')\n","plt.xlabel('epoch')\n","plt.show()\n"],"metadata":{"id":"I0IrfJs_Ud3l"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Fine tuning\n","In the feature extraction experiment, you were only training a few layers on top of an MobileNetV2 base model. The weights of the pre-trained network were not updated during training.\n","\n","One way to increase performance even further is to train (or \"fine-tune\") the weights of the top layers of the pre-trained model alongside the training of the classifier you added. The training process will force the weights to be tuned from generic feature maps to features associated specifically with the dataset."],"metadata":{"id":"iwwUq9uvWfma"}},{"cell_type":"markdown","source":["First, unfrezee the base model"],"metadata":{"id":"sTn1qHCSWsPb"}},{"cell_type":"code","source":["base_model.trainable = True"],"metadata":{"id":"JydUV-cmWX4-"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":[" set the bottom layers to be un-trainable. Then, you should recompile the model (necessary for these changes to take effect), and resume training."],"metadata":{"id":"-y0eJuT4W1K_"}},{"cell_type":"code","source":["# Let's take a look to see how many layers are in the base model\n","print(\"Number of layers in the base model: \", len(base_model.layers))\n","\n","# Fine-tune from this layer onwards\n","fine_tune_at = 100\n","\n","# Freeze all the layers before the `fine_tune_at` layer\n","for layer in base_model.layers[:fine_tune_at]:\n","  layer.trainable = False"],"metadata":{"id":"kk871VvlWwPu"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Compile the model to fine tune"],"metadata":{"id":"dWcYp383W9DD"}},{"cell_type":"code","source":["model.compile(loss='binary_crossentropy',\n","              optimizer = tf.keras.optimizers.RMSprop(learning_rate=base_learning_rate/10),\n","              metrics=['accuracy'])"],"metadata":{"id":"BEvOOS-sW5Wy"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Fine tune the model"],"metadata":{"id":"Fb_fJFmuXPZi"}},{"cell_type":"markdown","source":["We will continue training the model, if you trained to convergence earlier, this step will improve your accuracy by a few percentage points"],"metadata":{"id":"N5Od93U7XhJ7"}},{"cell_type":"code","source":["initial_epochs=4\n","fine_tune_epochs = 4\n","total_epochs =  initial_epochs + fine_tune_epochs\n","\n","history_fine = model.fit_generator(train_generator,\n","                         epochs=total_epochs,\n","                         initial_epoch=history.epoch[-1],\n","                         validation_data=valid_generator)"],"metadata":{"id":"D-qMYghzXNRr"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Lets plot the training curves"],"metadata":{"id":"VBpLstPCajSm"}},{"cell_type":"code","source":["acc += history_fine.history['accuracy']\n","val_acc += history_fine.history['val_accuracy']\n","\n","loss += history_fine.history['loss']\n","val_loss += history_fine.history['val_loss']\n","\n","plt.figure(figsize=(8, 8))\n","plt.subplot(2, 1, 1)\n","plt.plot(acc, label='Training Accuracy')\n","plt.plot(val_acc, label='Validation Accuracy')\n","plt.ylim([0.8, 1])\n","plt.plot([initial_epochs-1,initial_epochs-1],\n","          plt.ylim(), label='Start Fine Tuning')\n","plt.legend(loc='lower right')\n","plt.title('Training and Validation Accuracy')\n","\n","plt.subplot(2, 1, 2)\n","plt.plot(loss, label='Training Loss')\n","plt.plot(val_loss, label='Validation Loss')\n","plt.ylim([0, 1.0])\n","plt.plot([initial_epochs-1,initial_epochs-1],\n","         plt.ylim(), label='Start Fine Tuning')\n","plt.legend(loc='upper right')\n","plt.title('Training and Validation Loss')\n","plt.xlabel('epoch')\n","plt.show()"],"metadata":{"id":"eAPgBioWX3_H"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Evaluate the model"],"metadata":{"id":"cO3uFPVdarnM"}},{"cell_type":"code","source":["loss, accuracy = model.evaluate(test_generator)\n","print('Test accuracy :', accuracy)"],"metadata":{"id":"dDBRzMD0anL4"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"TTBKs8Nja3L7"},"execution_count":null,"outputs":[]}]}
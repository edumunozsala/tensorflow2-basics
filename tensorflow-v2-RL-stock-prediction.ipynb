{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"private_outputs":true,"provenance":[],"authorship_tag":"ABX9TyPmmeJDo7fAKDNwZyBYeXGI"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["## Tensorflow 2 Tutorial in Reinforcement Learning to predict stock prices"],"metadata":{"id":"-3enwgciL3Bc"}},{"cell_type":"markdown","source":["## Step 1: Loading the libraries"],"metadata":{"id":"3mqueuZVMFwp"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"BjNP4HolLXHE"},"outputs":[],"source":["!pip install pandas-datareader"]},{"cell_type":"code","source":["import tensorflow as tf\n","\n","import math\n","import random\n","import numpy as np\n","import pandas as pd\n","\n","import matplotlib.pyplot as plt\n","from pandas_datareader import data as pdr\n","import yfinance as yf\n","\n","\n","from tqdm import tqdm_notebook, tqdm\n","from collections import deque"],"metadata":{"id":"WWPd4ZMEMDRU"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(tf.__version__)"],"metadata":{"id":"Xuk52GGTMi2-"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Step 2: Build the neural network"],"metadata":{"id":"MwV3AxK9MrPQ"}},{"cell_type":"markdown","source":["First, we create an agent to work in our enviroment. It will use a FFN to generate the action and implement the RL reward and policy."],"metadata":{"id":"1pxPmJnAMvbb"}},{"cell_type":"code","source":["class AI_Trader():\n","\n","  def __init__(self, state_size, action_space=3, model_name=\"AITrader\"): #Manten, Compra, Vende\n","    # Set the state, action space and parameters\n","    self.state_size = state_size\n","    self.action_space = action_space\n","    self.memory = deque(maxlen=2000)\n","    self.inventory = []\n","    self.model_name = model_name\n","\n","    self.gamma = 0.95\n","    self.epsilon = 1.0\n","    self.epsilon_final = 0.01\n","    self.epsilon_decay = 0.995\n","\n","    self.model = self.model_builder()\n","\n","  def model_builder(self):\n","    # Create a FFN with three Dense layers and the output layer\n","    model = tf.keras.models.Sequential()\n","    model.add(tf.keras.layers.Dense(units=32, activation='relu', input_dim=self.state_size))\n","    model.add(tf.keras.layers.Dense(units=64, activation='relu'))\n","    model.add(tf.keras.layers.Dense(units=128, activation='relu'))\n","    model.add(tf.keras.layers.Dense(units=self.action_space, activation='linear'))\n","    model.compile(loss='mse', optimizer=tf.keras.optimizers.Adam(learning_rate=0.001))\n","\n","    return model\n","\n","  def trade(self, state):\n","    # Predict the next action based on the FFN\n","    if random.random() <= self.epsilon:\n","      return random.randrange(self.action_space)\n","\n","    actions = self.model.predict(state)\n","    return np.argmax(actions[0])\n","\n","\n","  def batch_train(self, batch_size):\n","    # Train the FFN using the reward function\n","    batch = []\n","    for i in range(len(self.memory) - batch_size + 1, len(self.memory)):\n","      batch.append(self.memory[i])\n","\n","    for state, action, reward, next_state, done in batch:\n","      reward = reward\n","      if not done:\n","        reward = reward + self.gamma * np.amax(self.model.predict(next_state)[0])\n","\n","      target = self.model.predict(state)\n","      target[0][action] = reward\n","\n","      self.model.fit(state, target, epochs=1, verbose=0)\n","\n","    if self.epsilon > self.epsilon_final:\n","      self.epsilon *= self.epsilon_decay"],"metadata":{"id":"qW-JuUTRMlAN"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Step 3: Data preprocessing"],"metadata":{"id":"iBVIf2GFObYN"}},{"cell_type":"markdown","source":["Create some helper functions"],"metadata":{"id":"iXQBg6BSOf3o"}},{"cell_type":"code","source":["# Calculate sigmoid function\n","def sigmoid(x):\n","  return 1 / (1 + math.exp(-x))\n","\n","# Set the stock price format\n","def stocks_price_format(n):\n","  if n < 0:\n","    return \"- $ {0:2f}\".format(abs(n))\n","  else:\n","    return \"$ {0:2f}\".format(abs(n))"],"metadata":{"id":"IXoPBF00OaaY"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Load the dataset"],"metadata":{"id":"Ay8Ct1ZmOu2y"}},{"cell_type":"code","source":["def dataset_loader(stock_names, start_date='' , end_date=''):\n","# Read the dataset\n","\n","  yf.pdr_override()\n","\n","  if start_date and end_date:\n","    # Load the sotkc prices between start and end dates\n","    dataset = pdr.get_data_yahoo(stock_names, start=start_date, end=end_date)\n","  else:\n","    # Load the whole stock prices set\n","    dataset = pdr.get_data_yahoo(stock_names)\n","\n","\n","  close = dataset['Close']\n","\n","  return close"],"metadata":{"id":"IBt9KqnLOt9t"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#stock_name = \"AAPL\"\n","stock_name = \"BTC-USD\"\n","data = dataset_loader(stock_name)\n","\n","print(data)"],"metadata":{"id":"d0mOeQPCVRRc"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Step 4: Train the agent"],"metadata":{"id":"1CcEpeP0bE9K"}},{"cell_type":"code","source":["# Return the state\n","def state_creator(data, timestep, window_size):\n","\n","  starting_id = timestep - window_size + 1\n","\n","  if starting_id >= 0:\n","    windowed_data = list(data[starting_id:timestep+1])\n","  else:\n","    windowed_data = - starting_id * [data[0]] + list(data[0:timestep+1])\n","\n","  state = []\n","  for i in range(window_size - 1):\n","    state.append(sigmoid(windowed_data[i+1] - windowed_data[i]))\n","\n","  return np.array([state])"],"metadata":{"id":"Co43t1TLPh2V"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Set the hyper parameters"],"metadata":{"id":"sxPbky1AbVqC"}},{"cell_type":"code","source":["window_size = 10\n","episodes = 1000\n","\n","batch_size = 32\n","data_samples = len(data) - 1"],"metadata":{"id":"dRg2wbkSbTIG"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Create the Agent or model"],"metadata":{"id":"B1m7bl1pbb4i"}},{"cell_type":"code","source":["trader = AI_Trader(window_size)"],"metadata":{"id":"FKpTy5fUbYvD"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["trader.model.summary()"],"metadata":{"id":"iLGI315Tben-"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Create the training loop"],"metadata":{"id":"7AiRkgBYbzHQ"}},{"cell_type":"code","source":["for episode in range(1, episodes + 1):\n","\n","  print(\"Episodio: {}/{}\".format(episode, episodes))\n","\n","  state = state_creator(data, 0, window_size + 1)\n","\n","  total_profit = 0\n","  trader.inventory = []\n","\n","  for t in tqdm(range(data_samples)):\n","\n","    action = trader.trade(state)\n","\n","    next_state = state_creator(data, t+1, window_size + 1)\n","    reward = 0\n","\n","    if action == 1: #Compra\n","      trader.inventory.append(data[t])\n","      print(\"AI Trader compró: \", stocks_price_format(data[t]))\n","\n","    elif action == 2 and len(trader.inventory) > 0: #Vende\n","      buy_price = trader.inventory.pop(0)\n","\n","      reward = max(data[t] - buy_price, 0)\n","      total_profit += data[t] - buy_price\n","      print(\"AI Trader vendió: \", stocks_price_format(data[t]), \" Beneficio: \" + stocks_price_format(data[t] - buy_price) )\n","\n","    if t == data_samples - 1:\n","      done = True\n","    else:\n","      done = False\n","\n","    trader.memory.append((state, action, reward, next_state, done))\n","\n","    state = next_state\n","\n","    if done:\n","      print(\"########################\")\n","      print(\"BENEFICIO TOTAL: {}\".format(total_profit))\n","      print(\"########################\")\n","\n","    if len(trader.memory) > batch_size:\n","      trader.batch_train(batch_size)\n","\n","  if episode % 10 == 0:\n","    trader.model.save(\"ai_trader_{}.h5\".format(episode))\n",""],"metadata":{"id":"q41ujEnwbwhQ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"CU9PyFbxb9Dk"},"execution_count":null,"outputs":[]}]}